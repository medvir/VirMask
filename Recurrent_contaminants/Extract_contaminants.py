#!/usr/bin/env python3
"""
This script identifies recurrent contaminants in the viral database.
1. Analyses all the mNGS runs provided with VirMet.
2. Checks all the unique.tsv files generated by VirMet.
3. Finds potential contaminants:
- Sequences that appear in more than 20% of our samples.
and / or:
- Sequences that appear in more than 20% of reads matching a specific ACC.

The idea is that these sequences are problably contamination, either
from the viral database, or introduced during the sample preparation 
(previous to sequencing). In either case, we don't want to classify
them as true positives.

Output: a .tsv table where the recurrent contaminants are shown.
"""

# Import standard libraries
import argparse
import math
import os
import re
import subprocess
import sys

# Import third-party libraries
import pandas as pd

# Parse command-line arguments
parser = argparse.ArgumentParser(
    description="Extract recurrent contaminant regions from VirMet outputs."
    )
parser.add_argument(
    "--mNGS_runs",
    type=str, 
    default="./mNGS_to_analyse.txt",
    help="Path to a .txt file listing all mNGS runs to consider (one run per row)"
)
parser.add_argument(
    "--virmet_db",
    type=str, 
    default="/data/virmet_databases_update/",
    help="Path to the VirMet database (default: /data/virmet_databases_update/)"
)
parser.add_argument(
    "--across_sample_freq",
    type=float,
    default=0.2,
    help="Across-sample frequency threshold (default: 0.2 = 20%%)"
)
parser.add_argument(
    "--within_acc_dominance",
    type=float,
    default=0.2,
    help="Within-accession dominance threshold (default: 0.2 = 20%%)"
)
parser.add_argument(
    "--controls",
    type=str,
    default="Tunavirus T1, phage MS2",
    help=(
        "Pattern to identify viruses that should be prevented from masking. "
        "If more than one is provided, add them within quotes and separated by commas. "
        "Example: 'Tunavirus T1, phage MS2'. "
        "This avoids masking your internal controls, which appear in all samples."
        )
)
args = parser.parse_args()

def find_folders_with_re(base_path, pattern):
    regex = re.compile(pattern)
    return [
        os.path.join(base_path, name) for name in os.listdir(base_path)
        if os.path.isdir(os.path.join(base_path, name)) and regex.match(name)
    ]

def find_subdir(base_folders):
    matched = []
    for base in base_folders:
        for root, dirs, _ in os.walk(base):
            for d in dirs:
                matched.append(os.path.join(root, d))
    return matched

def find_common_overlaps(df, col1, col2, threshold_percent):
    # Normalize intervals (start <= end)
    starts = df[[col1, col2]].min(axis=1).astype(int)
    ends_inclusive = df[[col1, col2]].max(axis=1).astype(int)

    ends = ends_inclusive + 1

    total = len(df)
    threshold = math.ceil(total * threshold_percent)

    # Collect all unique boundary points
    points = sorted(set(starts).union(set(ends)))

    candidate_regions = []
    for a, b in zip(points, points[1:]):
        if a < b:
            mask = (ends > a) & (starts < b)
            count = int(mask.sum())

            if count >= threshold:
                candidate_regions.append({
                    'start': a,
                    'end': b - 1,
                    'count': count,
                    'percentage': count / total,
                    'rows': df[mask].copy()
                })

    return candidate_regions

def merge_intervals(group):
    acc = group['ACC'].iloc[0]
    merged = []
    for _, row in group.iterrows():
        s, e = row['Start'], row['End']
        if not merged or s > merged[-1][1] + 1:
            merged.append([s, e, row['Species']])
        else:
            merged[-1][1] = max(merged[-1][1], e)
    df = pd.DataFrame(merged, columns=['Start', 'End', 'Species'])
    df.insert(0, 'ACC', acc)
    return df

def find_results(all_groups, perc_samples, perc_reads, all_data, min_reads):
    
    min_samples = len(all_data) * perc_samples
    within_acc = perc_reads

    data = []
    for key, group in all_groups.items():
        # Make sure that the ACC is found in >=x% samples
        if group['sample_idx'].nunique() >= max(min_samples, min_reads):
            # Find overalps that appear in more (or equal) than x% of the reads
            regions = find_common_overlaps(
                group, 
                'sstart', 
                'send', 
                threshold_percent=within_acc)
            # Take regions which are found in >x% samples
            for i, region in enumerate(regions):
                if region['rows']['sample_idx'].nunique() >= max(min_samples, min_reads):
                    ovr_row = {'Species': region['rows']['ssciname'].iloc[1],
                    'ACC': key,
                    'Counts' : region['count'],
                    'Start' : region['start'],
                    'End' : region['end']}
                    data.append(ovr_row)

    result_overlap = pd.DataFrame(data)
    return result_overlap

if __name__ == "__main__":

    with open(args.mNGS_runs) as f:
        to_reanalyse = [line.strip() for line in f if line.strip()]

    # Reanalyse using the input (unmasked or partly masked) database
    for analysis_folder in to_reanalyse:
        cmd = ['virmet', 'wolfpack', '--run', analysis_folder, 
        '--dbdir', args.virmet_db,
        '--nocovplot',
        '--noctrls']
        result = subprocess.run(cmd, capture_output=True, text=True)

    # Print stdout and stderr
    print("STDOUT:", result.stdout)
    print("STDERR:", result.stderr)
    
    # Find all the new virmet outputs
    virmet_new = find_folders_with_re('./', r'virmet_output_')

    # Take only clinical samples, excluding controls and undetermined
    virmet_folders = find_subdir(virmet_new)

    if len(virmet_folders) < 100:
        print('''Less than 100 samples are found.
        Masking of repetitive viral sequences not executed.
        Please, try again after adding more virmet outputs.''')
        sys.exit()

    # Get unique files
    unique_files = [s + "/unique.tsv.gz" for s in virmet_folders]

    # Read each file
    columns_to_use = ['qseqid', 'sseqid', 'ssciname', 'sstart', 'send']
    dfs = []

    for i, file in enumerate(unique_files):
        df = pd.read_csv(
            file,
            compression='gzip',
            sep='\t',
            header=0,
            usecols=columns_to_use
            )
        df['sample_idx'] = i
        dfs.append(df)

    # Concatenate all DataFrames
    combined_df = pd.concat(dfs, ignore_index=True)

    # Divide dataframe into smaller ones, each with a different Accession Num
    all_groups = dict(tuple(combined_df.groupby('sseqid')))

    # Define thresholds
    # Use 2 variants: 1 - take sequences that appear in >x% of samples
    # 2 - take sequences that appear in >x% of found reads for an ACCs
    # Ensure that the region is found at least 20 times before masking.
    # Do it separately because we want cond1 or cond2 (not cond1 & cond2)
    df1 = find_results(all_groups, args.across_sample_freq, 0, dfs, 0)
    df2 = find_results(all_groups, 0, args.within_acc_dominance, dfs, 20)
    
    # Concatenate the 2 tables
    combined_ovr = pd.concat([df1, df2], ignore_index=True)
    combined_ovr = combined_ovr.drop_duplicates()

    # Merge intervals that overlap for each ID
    combined_ovr = combined_ovr.sort_values(
        by=['ACC', 'Start']
        ).reset_index(drop=True)
    result = combined_ovr.groupby(
        'ACC')[['ACC', 'Start', 'End', 'Species']].apply(
            merge_intervals
            ).reset_index(drop=True)
    
    # Define patterns to exclude from masking (controls):
    if args.controls == "Tunavirus T1, phage MS2":
        pattern_ic_ms2 = ("Emesvirus zinderi", "phage MS2", "virus MS2")
        pattern_ic_t1 = ("Tunavirus", "phage T1", "virus T1")
        all_patterns = pattern_ic_ms2 + pattern_ic_t1
    else:
        all_patterns = tuple(item.strip() for item in args.controls.split(","))

    # compile regex pattern (escaped and joined with |)
    regex_pattern = "|".join(map(re.escape, all_patterns))

    result_wo_ctrls = result[
    ~(result['Species'].str.contains(regex_pattern, case=False, na=False))
    ].copy()

    result_wo_ctrls.to_csv('Recurrent_contaminant_regions.tsv', sep='\t', index=False)